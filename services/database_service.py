"""
Database Service ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PIPELINE_SQLSERVER

‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á SQL Server
"""

import logging
import os
from tkinter import messagebox
from typing import Any, Dict, Tuple

import pandas as pd
from sqlalchemy import create_engine, text
from sqlalchemy.exc import DBAPIError, ProgrammingError

from config.database import DatabaseConfig
from constants import DatabaseConstants, ErrorMessages, SuccessMessages
from .permission_checker_service import PermissionCheckerService

class DatabaseService:
    """
    ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• SQL Server
    
    ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ schema
    """
    
    def __init__(self) -> None:
        """
        ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô DatabaseService
        
        ‡∏™‡∏£‡πâ‡∏≤‡∏á database config, engine ‡πÅ‡∏•‡∏∞ logger
        """
        self.db_config = DatabaseConfig()
        self.engine = self.db_config.get_engine()
        self.logger = logging.getLogger(__name__)
        self.permission_checker = None  # ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≠‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

    def _get_permission_checker(self, log_callback=None):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ PermissionCheckerService"""
        if self.permission_checker is None:
            # ‡πÉ‡∏ä‡πâ silent callback ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GUI
            default_callback = log_callback or (lambda msg: None)
            self.permission_checker = PermissionCheckerService(
                engine=self.engine, 
                log_callback=default_callback
            )
        return self.permission_checker

    def check_permissions(self, schema_name: str = 'bronze', log_callback=None) -> Dict:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå SQL Server ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
        
        Args:
            schema_name: ‡∏ä‡∏∑‡πà‡∏≠ schema ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
            log_callback: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á log (None = ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á log)
            
        Returns:
            Dict: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå
        """
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GUI ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á log ‡πÉ‡∏ô CLI
        silent_callback = lambda msg: None  # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏á‡∏µ‡∏¢‡∏ö
        checker = self._get_permission_checker(silent_callback if log_callback is None else log_callback)
        return checker.check_all_permissions(schema_name)

    def generate_permission_report(self, schema_name: str = 'bronze') -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
        
        Args:
            schema_name: ‡∏ä‡∏∑‡πà‡∏≠ schema ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
            
        Returns:
            str: ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå
        """
        checker = self._get_permission_checker()
        return checker.generate_permission_report(schema_name)

    def check_connection(self, show_warning: bool = True) -> Tuple[bool, str]:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö SQL Server
        
        Returns:
            Tuple[bool, str]: (‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠, ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå)
        """
        try:
            with self.engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            return True, SuccessMessages.DB_CONNECTION_SUCCESS
        except Exception as e:
            error_msg = f"{ErrorMessages.DB_CONNECTION_FAILED}: {e}"
            self.logger.error(error_msg)
            if show_warning:
                messagebox.showwarning("Database connection", error_msg)
            return False, error_msg

    def test_connection(self, config: Dict[str, Any]) -> bool:
        """
        ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö SQL Server ‡∏î‡πâ‡∏ß‡∏¢ config ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏°‡∏≤
        
        Args:
            config (Dict[str, Any]): ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
            
        Returns:
            bool: True ‡∏´‡∏≤‡∏Å‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à, False ‡∏´‡∏≤‡∏Å‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß
        """
        try:
            driver = DatabaseConstants.DEFAULT_DRIVER
            if config["auth_type"] == DatabaseConstants.AUTH_WINDOWS:
                conn_str = (
                    f"mssql+pyodbc://@{config['server']}/{config['database']}?"
                    f"driver={driver}&Trusted_Connection=yes"
                )
            else:
                conn_str = (
                    f"mssql+pyodbc://{config['username']}:{config['password']}@{config['server']}/{config['database']}?"
                    f"driver={driver}"
                )
            engine = create_engine(conn_str)
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            return True
        except Exception as e:
            self.logger.error(f"Connection error: {e}")
            return False

    def update_config(self, server=None, database=None, auth_type=None, username=None, password=None):
        """‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠"""
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô DatabaseConfig
        self.db_config.update_config(
            server=server,
            database=database,
            auth_type=auth_type,
            username=username,
            password=password
        )
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï engine
        self.engine = self.db_config.get_engine()

    def ensure_schemas_exist(self, schema_names):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á schema ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏ ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ"""
        try:
            with self.engine.begin() as conn:
                for schema_name in schema_names:
                    conn.execute(text(f"""
                        IF NOT EXISTS (
                            SELECT 1 FROM sys.schemas WHERE name = '{schema_name}'
                        )
                        BEGIN
                            EXEC('CREATE SCHEMA {schema_name}')
                        END
                    """))
            return True, f"Verified/created schemas: {', '.join(schema_names)}"
        except Exception as e:
            error_msg = f"Failed to create schema: {e}"
            self.logger.error(error_msg)
            messagebox.showwarning("Schema creation", error_msg)
            return False, error_msg

    def _fix_text_columns_to_nvarchar_max(self, table_name, required_cols, schema_name='bronze', log_func=None):
        """‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Text() ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô NVARCHAR(MAX) ‡πÉ‡∏ô SQL Server"""
        from sqlalchemy.types import Text
        
        try:
            with self.engine.begin() as conn:
                for col_name, dtype in required_cols.items():
                    if isinstance(dtype, Text):
                        alter_sql = f"ALTER TABLE {schema_name}.{table_name} ALTER COLUMN [{col_name}] NVARCHAR(MAX)"
                        if log_func:
                            log_func(f"üîß Alter column '{col_name}' to NVARCHAR(MAX)")
                        conn.execute(text(alter_sql))
        except Exception as e:
            if log_func:
                log_func(f"‚ö†Ô∏è Unable to alter Text() column: {e}")

    def upload_data(self, df, logic_type, required_cols, schema_name='bronze', log_func=None, force_recreate=False):
        """
        ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏ï‡∏≤‡∏° config, insert ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ, ‡∏ñ‡πâ‡∏≤ schema DB ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡πÉ‡∏´‡πâ drop ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
        
        Args:
            df: DataFrame ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î
            logic_type: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
            required_cols: ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏•‡∏∞‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            schema_name: ‡∏ä‡∏∑‡πà‡∏≠ schema ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            log_func: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö log
            force_recreate: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà (‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)
        """
        
        # ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏ú‡πà‡∏≤‡∏ô ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
        if log_func:
            log_func("‚úÖ Database access permissions are correct")
        
        try:
            import json
            from datetime import datetime
            from sqlalchemy.types import DateTime
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
            if df is None or df.empty:
                return False, "Empty data"
            
            if not required_cols:
                return False, "Data type settings not found"
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå timestamp
            current_time = datetime.now()
            df['updated_at'] = current_time
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° dtype ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå timestamp
            required_cols['updated_at'] = DateTime()
            
            table_name = None
            try:
                with open('config/column_settings.json', 'r', encoding='utf-8') as f:
                    col_config = json.load(f)
                table_name = col_config.get("__table_names__", {}).get(logic_type)
            except Exception:
                table_name = None
            if not table_name:
                table_name = logic_type

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á schema ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
            schema_result = self.ensure_schemas_exist([schema_name])
            if not schema_result[0]:
                return False, f"Could not create schema: {schema_result[1]}"

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö schema DB ‡∏ß‡πà‡∏≤‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö required_cols ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            from sqlalchemy import inspect
            from sqlalchemy.types import Text
            insp = inspect(self.engine)
            needs_recreate = force_recreate
            
            if insp.has_table(table_name, schema=schema_name) and not force_recreate:
                db_cols = [col['name'] for col in insp.get_columns(table_name, schema=schema_name)]
                db_col_types = {col['name']: str(col['type']).upper() for col in insp.get_columns(table_name, schema=schema_name)}
                config_cols = list(required_cols.keys())
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
                if set(db_cols) != set(config_cols):
                    msg = f"‚ùå Table {schema_name}.{table_name} columns do not match config"
                    needs_recreate = True
                else:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö data types ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
                    from sqlalchemy.types import NVARCHAR as SA_NVARCHAR, DateTime as SA_DateTime, DATE as SA_DATE
                    from sqlalchemy.types import Integer as SA_Integer, SmallInteger as SA_SmallInteger, Float as SA_Float, DECIMAL as SA_DECIMAL, Text as SA_Text, Boolean as SA_Boolean

                    def _type_category(type_str: str) -> str:
                        t = (type_str or '').upper()
                        if any(x in t for x in ['INT', 'BIGINT', 'SMALLINT', 'FLOAT', 'DECIMAL', 'NUMERIC', 'REAL', 'MONEY']):
                            return 'NUMERIC'
                        if any(x in t for x in ['DATE', 'DATETIME', 'SMALLDATETIME', 'DATETIME2', 'TIME']):
                            return 'DATETIME'
                        if any(x in t for x in ['BIT', 'BOOLEAN']):
                            return 'BOOLEAN'
                        if any(x in t for x in ['CHAR', 'NCHAR', 'VARCHAR', 'NVARCHAR', 'TEXT']):
                            return 'STRING'
                        return 'OTHER'

                    def _expected_type_str(sa_type_obj) -> str:
                        # ‡πÅ‡∏õ‡∏•‡∏á SQLAlchemy type object ‡πÄ‡∏õ‡πá‡∏ô string ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö DB
                        s = str(sa_type_obj).upper()
                        # Map Text() ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏õ‡πá‡∏ô NVARCHAR(MAX)
                        if isinstance(sa_type_obj, SA_Text):
                            return 'NVARCHAR(MAX)'
                        return s

                    def _parse_varchar_len(type_str: str) -> int:
                        try:
                            if 'MAX' in type_str.upper():
                                return 2_147_483_647  # ‡πÉ‡∏´‡∏ç‡πà‡∏û‡∏≠‡πÅ‡∏ó‡∏ô MAX
                            if '(' in type_str and ')' in type_str:
                                return int(type_str.split('(')[1].split(')')[0])
                        except Exception:
                            pass
                        return -1

                    for col_name, expected_dtype in required_cols.items():
                        db_type = db_col_types.get(col_name, '')
                        expected_str = _expected_type_str(expected_dtype)
                        cat_db = _type_category(db_type)
                        cat_expected = _type_category(expected_str)

                        # ‡∏Å‡∏£‡∏ì‡∏µ Text() ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô NVARCHAR(MAX) - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á
                        if isinstance(expected_dtype, SA_Text):
                            if 'NVARCHAR(MAX)' not in db_type and 'TEXT' not in db_type and 'NVARCHAR' not in db_type:
                                # ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà NVARCHAR ‡πÄ‡∏•‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô INT, DATE)
                                if log_func:
                                    log_func(f"‚ùå Schema Mismatch: Column '{col_name}' should be NVARCHAR(MAX) but database has {db_type}")
                                    log_func(f"   üí° Suggestion: Allow system to create new table to fix this issue")
                                needs_recreate = True
                                break

                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö mismatch ‡∏Ç‡∏≠‡∏á‡∏´‡∏°‡∏ß‡∏î‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÄ‡∏ä‡πà‡∏ô STRING ‚Üî NUMERIC)
                        if cat_db != cat_expected:
                            if log_func:
                                log_func(f"‚ùå Data type mismatch for column '{col_name}' (DB: {db_type} | Expected: {expected_str})")
                            needs_recreate = True
                            break

                        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô STRING ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß NVARCHAR
                        if cat_expected == 'STRING' and 'NVARCHAR' in expected_str:
                            exp_len = _parse_varchar_len(expected_str)
                            act_len = _parse_varchar_len(db_type)
                            if act_len != -1 and exp_len != -1 and act_len < exp_len:
                                if log_func:
                                    log_func(f"‚ùå NVARCHAR length for '{col_name}' is insufficient (DB: {db_type} | Expected: {expected_str})")
                                needs_recreate = True
                                break
            
            # ------------------------------
            # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÉ‡∏´‡∏°‡πà: Ingest ‚Üí NVARCHAR(MAX) staging ‚Üí SQL ‡πÅ‡∏õ‡∏•‡∏á‡∏´‡∏•‡∏±‡∏á‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤
            # ------------------------------
            # 1) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á staging ‡∏î‡πâ‡∏ß‡∏¢ NVARCHAR(MAX) ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            staging_table = f"{table_name}__stg"
            staging_cols = list(required_cols.keys())
            with self.engine.begin() as conn:
                # ‡∏•‡∏ö staging table ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
                conn.execute(text(f"""
                    IF OBJECT_ID('{schema_name}.{staging_table}', 'U') IS NOT NULL
                        DROP TABLE {schema_name}.{staging_table};
                """))
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á staging table ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô NVARCHAR(MAX) ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
                cols_sql = ", ".join([f"[{c}] NVARCHAR(MAX) NULL" for c in staging_cols])
                conn.execute(text(f"CREATE TABLE {schema_name}.{staging_table} ({cols_sql})"))
                if log_func:
                    log_func(f"üì¶ Created staging table: {schema_name}.{staging_table} (NVARCHAR(MAX) for all columns)")

            # 2) ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤ staging ‡∏î‡πâ‡∏ß‡∏¢ pandas.to_sql
            if len(df) > 10000:
                if log_func:
                    log_func(f"üìä Large file ({len(df):,} rows) - uploading in chunks to staging")
                chunk_size = 5000
                total_chunks = (len(df) + chunk_size - 1) // chunk_size
                for i in range(0, len(df), chunk_size):
                    chunk = df.iloc[i:i+chunk_size]
                    chunk[staging_cols].to_sql(
                        name=staging_table,
                        con=self.engine,
                        schema=schema_name,
                        if_exists='append',
                        index=False
                    )
                    chunk_num = (i // chunk_size) + 1
                    if log_func:
                        log_func(f"üì§ Uploaded staging chunk {chunk_num}/{total_chunks}: {len(chunk):,} rows")
            else:
                if log_func:
                    log_func(f"üì§ Uploaded data: {len(df):,} rows ‚Üí {schema_name}.{staging_table}")
                df[staging_cols].to_sql(
                    name=staging_table,
                    con=self.engine,
                    schema=schema_name,
                    if_exists='append',
                    index=False
                )

            # 3) ‡∏™‡∏£‡πâ‡∏≤‡∏á/‡∏£‡∏µ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á‡∏ï‡∏≤‡∏° dtype config (‡πÑ‡∏°‡πà auto-fix)
            if needs_recreate or not insp.has_table(table_name, schema=schema_name):
                if needs_recreate and log_func:
                    log_func(f"üõ†Ô∏è Creating table {schema_name}.{table_name} to match data type settings")
                elif log_func:
                    log_func(f"üìã Creating table {schema_name}.{table_name} from data type settings")
                
                df.head(0)[list(required_cols.keys())].to_sql(
                    name=table_name,
                    con=self.engine,
                    schema=schema_name,
                    if_exists='replace',
                    index=False,
                    dtype=required_cols
                )
                # ‡∏õ‡∏£‡∏±‡∏ö Text() ‚Üí NVARCHAR(MAX)
                self._fix_text_columns_to_nvarchar_max(table_name, required_cols, schema_name, log_func)
            else:
                # ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á
                if log_func:
                    log_func(f"üßπ Truncating existing data in table {schema_name}.{table_name}")
                with self.engine.begin() as conn:
                    conn.execute(text(f"TRUNCATE TABLE {schema_name}.{table_name}"))

            # 4) ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô staging table ‡∏î‡πâ‡∏ß‡∏¢ SQL validation
            if log_func:
                log_func(f"üîç Validating data in staging table...")
            
            validation_results = self.validate_data_in_staging(staging_table, logic_type, required_cols, schema_name, log_func)
            
            if not validation_results['is_valid']:
                # ‡∏•‡∏ö staging table ‡πÅ‡∏•‡∏∞‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
                with self.engine.begin() as conn:
                    conn.execute(text(f"DROP TABLE {schema_name}.{staging_table}"))
                return False, validation_results['summary']
            
            # 5) ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å staging ‚Üí ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á ‡∏î‡πâ‡∏ß‡∏¢ TRY_CONVERT/REPLACE ‡∏ï‡∏≤‡∏° dtype ‡πÄ‡∏î‡∏¥‡∏°
            from sqlalchemy.types import (
                Integer as SA_Integer,
                SmallInteger as SA_SmallInteger,
                Float as SA_Float,
                DECIMAL as SA_DECIMAL,
                DATE as SA_DATE,
                DateTime as SA_DateTime,
                NVARCHAR as SA_NVARCHAR,
                Text as SA_Text,
                Boolean as SA_Boolean,
            )

            def _sql_type_and_expr(col_name: str, sa_type_obj) -> str:
                col_ref = f"[{col_name}]"
                base = "NULLIF(LTRIM(RTRIM(" + col_ref + ")), '')"
                if isinstance(sa_type_obj, (SA_Integer, SA_SmallInteger)):
                    return f"TRY_CONVERT(INT, REPLACE(REPLACE({base}, ',', ''), ' ', ''))"
                if isinstance(sa_type_obj, SA_Float):
                    return f"TRY_CONVERT(FLOAT, REPLACE(REPLACE({base}, ',', ''), ' ', ''))"
                if isinstance(sa_type_obj, SA_DECIMAL):
                    precision = getattr(sa_type_obj, 'precision', 18) or 18
                    scale = getattr(sa_type_obj, 'scale', 2) or 2
                    return f"TRY_CONVERT(DECIMAL({precision},{scale}), REPLACE(REPLACE({base}, ',', ''), ' ', ''))"
                if isinstance(sa_type_obj, (SA_DATE, SA_DateTime)):
                    # ‡∏•‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: ISO(121), UK(103), US(101)
                    return (
                        f"COALESCE("
                        f"TRY_CONVERT(DATETIME, {base}, 121),"
                        f"TRY_CONVERT(DATETIME, {base}, 103),"
                        f"TRY_CONVERT(DATETIME, {base}, 101)"
                        f")"
                    )
                if isinstance(sa_type_obj, SA_Boolean):
                    return (
                        "CASE "
                        f"WHEN UPPER(LTRIM(RTRIM({col_ref}))) IN ('1','TRUE','Y','YES') THEN 1 "
                        f"WHEN UPPER(LTRIM(RTRIM({col_ref}))) IN ('0','FALSE','N','NO') THEN 0 "
                        "ELSE NULL END"
                    )
                # NVARCHAR(n) / NVARCHAR(MAX)
                target = 'NVARCHAR(MAX)' if isinstance(sa_type_obj, SA_Text) else str(sa_type_obj).upper()
                return f"TRY_CONVERT({target}, {col_ref})"

            select_exprs = []
            for col_name, sa_type in required_cols.items():
                select_exprs.append(f"{_sql_type_and_expr(col_name, sa_type)} AS [{col_name}]")
            select_sql = ", ".join(select_exprs)

            with self.engine.begin() as conn:
                insert_sql = (
                    f"INSERT INTO {schema_name}.{table_name} (" + ", ".join([f"[{c}]" for c in required_cols.keys()]) + ") "
                    f"SELECT {select_sql} FROM {schema_name}.{staging_table}"
                )
                conn.execute(text(insert_sql))

                # ‡∏•‡∏ö staging table ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô
                conn.execute(text(f"DROP TABLE {schema_name}.{staging_table}"))
                if log_func:
                    log_func(f"üóëÔ∏è Dropped staging table {schema_name}.{staging_table}")

            return True, f"Upload successful ‚Üí {schema_name}.{table_name} (ingested NVARCHAR(MAX) then converted by dtype for {len(df):,} rows)"
        except Exception as e:
            # ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏ä‡∏µ‡πâ‡πÄ‡∏õ‡πâ‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
            def _short_exception_message(exc: Exception) -> str:
                try:
                    if isinstance(exc, DBAPIError) and getattr(exc, "orig", None):
                        # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏à‡∏≤‡∏Å DBAPI/pyodbc ‡πÇ‡∏î‡∏¢‡∏ï‡∏±‡∏î SQL ‡πÅ‡∏•‡∏∞ parameters ‡∏ó‡∏¥‡πâ‡∏á
                        orig = exc.orig
                        # pyodbc ‡∏à‡∏∞‡∏°‡∏µ args ‡πÄ‡∏õ‡πá‡∏ô tuple ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß ‡πÄ‡∏£‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏î
                        parts = [str(p) for p in getattr(orig, "args", []) if p]
                        core = parts[-1] if parts else str(exc)
                    else:
                        core = str(exc)
                    # ‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô [SQL: ...] ‡πÅ‡∏•‡∏∞ [parameters: ...] ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
                    for token in ["[SQL:", "[parameters:"]:
                        if token in core:
                            core = core.split(token)[0].strip()
                    # ‡∏¢‡πà‡∏≠‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô (‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å)
                    core = core.splitlines()[0]
                    return core
                except Exception:
                    return str(exc)

            def _detect_problem_columns(df_local, required_map, max_cols: int = 5, max_examples: int = 3):
                import pandas as pd  # ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á import ‡∏ß‡∏ô
                from sqlalchemy.types import (
                    Integer as SA_Integer,
                    SmallInteger as SA_SmallInteger,
                    Float as SA_Float,
                    DECIMAL as SA_DECIMAL,
                    DATE as SA_DATE,
                    DateTime as SA_DateTime,
                    NVARCHAR as SA_NVARCHAR,
                )
                problems = []
                try:
                    for col, dtype in required_map.items():
                        if col not in df_local.columns:
                            continue
                        # ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
                        if isinstance(dtype, (SA_Integer, SA_SmallInteger, SA_Float, SA_DECIMAL)):
                            numeric_series = pd.to_numeric(df_local[col], errors="coerce")
                            mask = numeric_series.isna() & df_local[col].notna()
                            bad_count = int(mask.sum())
                            if bad_count > 0:
                                examples = [str(x) for x in df_local.loc[mask, col].head(max_examples).tolist()]
                                problems.append({
                                    "column": col,
                                    "expected": str(dtype),
                                    "bad_count": bad_count,
                                    "examples": examples,
                                })
                        # ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
                        elif isinstance(dtype, (SA_DATE, SA_DateTime)):
                            parsed = pd.to_datetime(df_local[col], errors="coerce")
                            mask = parsed.isna() & df_local[col].notna()
                            bad_count = int(mask.sum())
                            if bad_count > 0:
                                examples = [str(x) for x in df_local.loc[mask, col].head(max_examples).tolist()]
                                problems.append({
                                    "column": col,
                                    "expected": str(dtype),
                                    "bad_count": bad_count,
                                    "examples": examples,
                                })
                        # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á string (‡∏ñ‡πâ‡∏≤‡∏Å‡∏≥‡∏´‡∏ô‡∏î NVARCHAR(n))
                        elif isinstance(dtype, SA_NVARCHAR) and getattr(dtype, "length", None):
                            max_len = int(dtype.length)
                            str_series = df_local[col].astype(str)
                            mask = str_series.str.len() > max_len
                            bad_count = int(mask.sum())
                            if bad_count > 0:
                                examples = str_series.loc[mask].str[:50].head(max_examples).tolist()
                                problems.append({
                                    "column": col,
                                    "expected": f"NVARCHAR({max_len})",
                                    "bad_count": bad_count,
                                    "examples": examples,
                                })
                        # ‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ
                        if len(problems) >= max_cols:
                            break
                except Exception:
                    pass
                return problems

            short_msg = _short_exception_message(e)
            problem_hints = _detect_problem_columns(df, required_cols)

            if problem_hints:
                lines = [
                    f"Database error: {short_msg}",
                    "Likely problematic columns (partial):",
                ]
                for p in problem_hints:
                    ex = ", ".join(p.get("examples", []))
                    lines.append(f"- {p['column']} (expected {p['expected']}) invalid {p['bad_count']:,} rows. Examples: [{ex}]")
                error_msg = "\n".join(lines)
            else:
                error_msg = f"Database error: {short_msg}"

            if log_func:
                log_func(f"‚ùå {error_msg}")
            return False, error_msg

    def validate_data_in_staging(self, staging_table, logic_type, required_cols, schema_name='bronze', log_func=None):
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô staging table ‡∏î‡πâ‡∏ß‡∏¢ SQL
        
        Args:
            staging_table: ‡∏ä‡∏∑‡πà‡∏≠ staging table
            logic_type: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
            required_cols: ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏•‡∏∞‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            schema_name: ‡∏ä‡∏∑‡πà‡∏≠ schema
            log_func: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö log
            
        Returns:
            Dict: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö {'is_valid': bool, 'issues': [...], 'summary': str}
        """
        try:
            from sqlalchemy.types import (
                Integer as SA_Integer,
                SmallInteger as SA_SmallInteger,
                Float as SA_Float,
                DECIMAL as SA_DECIMAL,
                DATE as SA_DATE,
                DateTime as SA_DateTime,
                NVARCHAR as SA_NVARCHAR,
                Text as SA_Text,
                Boolean as SA_Boolean,
            )
            
            validation_results = {
                'is_valid': True,
                'issues': [],
                'warnings': [],
                'summary': ''
            }
            
            total_rows = 0
            with self.engine.connect() as conn:
                # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                result = conn.execute(text(f"SELECT COUNT(*) FROM {schema_name}.{staging_table}"))
                total_rows = result.scalar()
            
            if total_rows == 0:
                validation_results['is_valid'] = False
                validation_results['summary'] = "No data in staging table"
                return validation_results
            
            if log_func:
                log_func(f"üìä Validating {total_rows:,} rows in staging table")
            
            validation_queries = self._build_validation_queries(staging_table, required_cols, schema_name)
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö schema mismatch
            schema_issues = self._check_schema_mismatch_in_staging(staging_table, required_cols, schema_name, log_func)
            if schema_issues:
                validation_results['warnings'].extend(schema_issues)
            
            with self.engine.connect() as conn:
                for validation_type, query in validation_queries.items():
                    try:
                        result = conn.execute(text(query))
                        rows = result.fetchall()
                        
                        if rows:
                            for row in rows:
                                issue = {
                                    'validation_type': validation_type,
                                    'column': row.column_name if hasattr(row, 'column_name') else 'unknown',
                                    'error_count': row.error_count if hasattr(row, 'error_count') else 0,
                                    'percentage': round((row.error_count / total_rows) * 100, 2) if hasattr(row, 'error_count') else 0,
                                    'examples': row.examples if hasattr(row, 'examples') else ''
                                }
                                
                                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á
                                if issue['percentage'] > 10:  # ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 10% ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á
                                    validation_results['is_valid'] = False
                                    validation_results['issues'].append(issue)
                                elif issue['percentage'] > 1:  # 1-10% ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô
                                    validation_results['warnings'].append(issue)
                                
                                if log_func and issue['error_count'] > 0:
                                    status = "‚ùå" if issue['percentage'] > 10 else "‚ö†Ô∏è"
                                    column_name = issue['column'] if isinstance(issue['column'], str) else str(issue['column'])
                                    examples = issue['examples'][:100] if isinstance(issue['examples'], str) else str(issue['examples'])[:100]
                                    log_func(f"   {status} {column_name}: {issue['error_count']:,} invalid rows ({issue['percentage']}%) Examples: {examples}")
                    
                    except Exception as query_error:
                        if log_func:
                            log_func(f"‚ö†Ô∏è Could not run validation query for {validation_type}: {query_error}")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
            if not validation_results['is_valid']:
                serious_issues = len(validation_results['issues'])
                warnings_count = len(validation_results['warnings'])
                validation_results['summary'] = f"Found {serious_issues} serious issues and {warnings_count} warnings - cannot import data"
            elif validation_results['warnings']:
                warnings_count = len(validation_results['warnings'])
                validation_results['summary'] = f"Found {warnings_count} warnings; data can be imported"
                if log_func:
                    log_func(f"‚úÖ Data validation passed (with {warnings_count} warnings)")
            else:
                validation_results['summary'] = "All data valid"
                if log_func:
                    log_func(f"‚úÖ All data passed validation")
                    
            return validation_results
            
        except Exception as e:
            validation_results = {
                'is_valid': False,
                'issues': [],
                'warnings': [],
                'summary': f"Error validating data: {str(e)}"
            }
            if log_func:
                log_func(f"‚ùå {validation_results['summary']}")
            return validation_results
    
    def _build_validation_queries(self, staging_table, required_cols, schema_name):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á SQL queries ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö validation ‡∏ï‡∏≤‡∏°‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
        """
        from sqlalchemy.types import (
            Integer as SA_Integer,
            SmallInteger as SA_SmallInteger,
            Float as SA_Float,
            DECIMAL as SA_DECIMAL,
            DATE as SA_DATE,
            DateTime as SA_DateTime,
            NVARCHAR as SA_NVARCHAR,
            Text as SA_Text,
            Boolean as SA_Boolean,
        )
        
        queries = {}
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        numeric_columns = []
        for col, dtype in required_cols.items():
            if isinstance(dtype, (SA_Integer, SA_SmallInteger, SA_Float, SA_DECIMAL)):
                numeric_columns.append(col)
        
        if numeric_columns:
            numeric_cases = []
            for col in numeric_columns:
                # ‡πÉ‡∏ä‡πâ NVARCHAR literal ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
                col_literal = f"N'{col}'" if any(ord(c) > 127 for c in col) else f"'{col}'"
                numeric_cases.append(f"""
                    SELECT {col_literal} as column_name, [{col}] as invalid_value, ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) as row_num
                    FROM {schema_name}.{staging_table}
                    WHERE TRY_CAST(REPLACE(REPLACE(NULLIF(LTRIM(RTRIM([{col}])), ''), ',', ''), ' ', '') AS FLOAT) IS NULL 
                      AND NULLIF(LTRIM(RTRIM([{col}])), '') IS NOT NULL
                """)
            
            queries['numeric_validation'] = f"""
                WITH numeric_errors AS (
                    {' UNION ALL '.join(numeric_cases)}
                ),
                error_counts AS (
                    SELECT column_name, COUNT(*) as error_count
                    FROM numeric_errors
                    GROUP BY column_name
                ),
                limited_examples AS (
                    SELECT column_name, invalid_value, row_num,
                           ROW_NUMBER() OVER (PARTITION BY column_name ORDER BY row_num) as rn
                    FROM numeric_errors
                )
                SELECT 
                    ec.column_name,
                    ec.error_count,
                    CAST(STRING_AGG(CAST(le.invalid_value AS NVARCHAR(50)), ', ') WITHIN GROUP (ORDER BY le.row_num) AS NVARCHAR(MAX)) as examples
                FROM error_counts ec
                LEFT JOIN limited_examples le ON ec.column_name = le.column_name AND le.rn <= 5
                GROUP BY ec.column_name, ec.error_count
            """
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
        date_columns = []
        for col, dtype in required_cols.items():
            if isinstance(dtype, (SA_DATE, SA_DateTime)):
                date_columns.append(col)
        
        if date_columns:
            date_cases = []
            for col in date_columns:
                # ‡πÉ‡∏ä‡πâ NVARCHAR literal ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
                col_literal = f"N'{col}'" if any(ord(c) > 127 for c in col) else f"'{col}'"
                date_cases.append(f"""
                    SELECT {col_literal} as column_name, [{col}] as invalid_value, ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) as row_num
                    FROM {schema_name}.{staging_table}
                    WHERE COALESCE(
                        TRY_CONVERT(DATETIME, NULLIF(LTRIM(RTRIM([{col}])), ''), 121),
                        TRY_CONVERT(DATETIME, NULLIF(LTRIM(RTRIM([{col}])), ''), 103),
                        TRY_CONVERT(DATETIME, NULLIF(LTRIM(RTRIM([{col}])), ''), 101)
                    ) IS NULL
                    AND NULLIF(LTRIM(RTRIM([{col}])), '') IS NOT NULL
                """)
            
            queries['date_validation'] = f"""
                WITH date_errors AS (
                    {' UNION ALL '.join(date_cases)}
                ),
                error_counts AS (
                    SELECT column_name, COUNT(*) as error_count
                    FROM date_errors
                    GROUP BY column_name
                ),
                limited_examples AS (
                    SELECT column_name, invalid_value, row_num,
                           ROW_NUMBER() OVER (PARTITION BY column_name ORDER BY row_num) as rn
                    FROM date_errors
                )
                SELECT 
                    ec.column_name,
                    ec.error_count,
                    CAST(STRING_AGG(CAST(le.invalid_value AS NVARCHAR(50)), ', ') WITHIN GROUP (ORDER BY le.row_num) AS NVARCHAR(MAX)) as examples
                FROM error_counts ec
                LEFT JOIN limited_examples le ON ec.column_name = le.column_name AND le.rn <= 5
                GROUP BY ec.column_name, ec.error_count
            """
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á string (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ NVARCHAR ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß)
        string_columns = []
        for col, dtype in required_cols.items():
            if isinstance(dtype, SA_NVARCHAR) and hasattr(dtype, 'length') and dtype.length:
                string_columns.append((col, dtype.length))
        
        if string_columns:
            string_cases = []
            for col, max_length in string_columns:
                # ‡πÉ‡∏ä‡πâ NVARCHAR literal ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
                col_literal = f"N'{col}'" if any(ord(c) > 127 for c in col) else f"'{col}'"
                string_cases.append(f"""
                    SELECT {col_literal} as column_name, LEFT([{col}], 50) + '...' as invalid_value, ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) as row_num
                    FROM {schema_name}.{staging_table}
                    WHERE LEN(ISNULL([{col}], '')) > {max_length}
                """)
            
            queries['string_length_validation'] = f"""
                WITH string_errors AS (
                    {' UNION ALL '.join(string_cases)}
                ),
                error_counts AS (
                    SELECT column_name, COUNT(*) as error_count
                    FROM string_errors
                    GROUP BY column_name
                ),
                limited_examples AS (
                    SELECT column_name, invalid_value, row_num,
                           ROW_NUMBER() OVER (PARTITION BY column_name ORDER BY row_num) as rn
                    FROM string_errors
                )
                SELECT 
                    ec.column_name,
                    ec.error_count,
                    CAST(STRING_AGG(CAST(le.invalid_value AS NVARCHAR(100)), ', ') WITHIN GROUP (ORDER BY le.row_num) AS NVARCHAR(MAX)) as examples
                FROM error_counts ec
                LEFT JOIN limited_examples le ON ec.column_name = le.column_name AND le.rn <= 3
                GROUP BY ec.column_name, ec.error_count
            """
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• boolean
        boolean_columns = []
        for col, dtype in required_cols.items():
            if isinstance(dtype, SA_Boolean):
                boolean_columns.append(col)
        
        if boolean_columns:
            boolean_cases = []
            for col in boolean_columns:
                # ‡πÉ‡∏ä‡πâ NVARCHAR literal ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
                col_literal = f"N'{col}'" if any(ord(c) > 127 for c in col) else f"'{col}'"
                boolean_cases.append(f"""
                    SELECT {col_literal} as column_name, [{col}] as invalid_value, ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) as row_num
                    FROM {schema_name}.{staging_table}
                    WHERE UPPER(LTRIM(RTRIM(ISNULL([{col}], '')))) NOT IN ('1','TRUE','Y','YES','0','FALSE','N','NO','')
                """)
            
            queries['boolean_validation'] = f"""
                WITH boolean_errors AS (
                    {' UNION ALL '.join(boolean_cases)}
                ),
                error_counts AS (
                    SELECT column_name, COUNT(*) as error_count
                    FROM boolean_errors
                    GROUP BY column_name
                ),
                limited_examples AS (
                    SELECT column_name, invalid_value, row_num,
                           ROW_NUMBER() OVER (PARTITION BY column_name ORDER BY row_num) as rn
                    FROM boolean_errors
                )
                SELECT 
                    ec.column_name,
                    ec.error_count,
                    CAST(STRING_AGG(CAST(le.invalid_value AS NVARCHAR(50)), ', ') WITHIN GROUP (ORDER BY le.row_num) AS NVARCHAR(MAX)) as examples
                FROM error_counts ec
                LEFT JOIN limited_examples le ON ec.column_name = le.column_name AND le.rn <= 5
                GROUP BY ec.column_name, ec.error_count
            """
        
        return queries
    
    def _check_schema_mismatch_in_staging(self, staging_table, required_cols, schema_name, log_func=None):
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö schema mismatch ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ staging table ‡πÅ‡∏•‡πâ‡∏ß
        ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£ validation ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
        """
        from sqlalchemy.types import Text as SA_Text
        
        schema_issues = []
        
        try:
            with self.engine.connect() as conn:
                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• schema ‡∏Ç‡∏≠‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                final_table = staging_table.replace('_staging', '')
                table_info_query = f"""
                    SELECT COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH, COLLATION_NAME
                    FROM INFORMATION_SCHEMA.COLUMNS 
                    WHERE TABLE_SCHEMA = '{schema_name}' 
                    AND TABLE_NAME = '{final_table}'
                """
                
                result = conn.execute(text(table_info_query))
                db_columns = {row.COLUMN_NAME: {
                    'data_type': row.DATA_TYPE,
                    'max_length': row.CHARACTER_MAXIMUM_LENGTH,
                    'collation': row.COLLATION_NAME
                } for row in result.fetchall()}
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö schema mismatch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
                for col_name, expected_dtype in required_cols.items():
                    if col_name in db_columns:
                        db_info = db_columns[col_name]
                        
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Text (NVARCHAR(MAX)) vs NVARCHAR ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
                        if isinstance(expected_dtype, SA_Text):
                            if db_info['data_type'] == 'nvarchar' and db_info['max_length'] != -1:
                                # NVARCHAR(MAX) in SQL Server has max_length = -1
                                issue = {
                                    'validation_type': 'schema_mismatch',
                                    'column': col_name,
                                    'error_count': 0,  # not an error, informational warning
                                    'percentage': 0,
                                    'message': f"Configured as NVARCHAR(MAX) but database column is NVARCHAR({db_info['max_length'] or 'Unknown'})",
                                    'recommendation': "Data will be saved, but may be truncated if it exceeds column length",
                                    'severity': 'info'
                                }
                                
                                schema_issues.append(issue)
                                
                                if log_func:
                                    log_func(f"   ‚ö†Ô∏è  {col_name}: {issue['message']}")
                                    log_func(f"      ‚ÑπÔ∏è  {issue['recommendation']}")
        
        except Exception as e:
            if log_func:
                log_func(f"‚ö†Ô∏è Unable to check schema mismatch: {e}")
        
        return schema_issues
