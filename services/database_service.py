"""
Database Service ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PIPELINE_SQLSERVER

‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á SQL Server
"""

import logging
import os
from tkinter import messagebox
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
from sqlalchemy import create_engine, text
from sqlalchemy.exc import DBAPIError, ProgrammingError

from config.database import DatabaseConfig
from constants import DatabaseConstants, ErrorMessages, SuccessMessages
from .permission_checker_service import PermissionCheckerService

class DatabaseService:
    """
    ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• SQL Server
    
    ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ schema
    """
    
    def __init__(self) -> None:
        """
        ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô DatabaseService
        
        ‡∏™‡∏£‡πâ‡∏≤‡∏á database config, engine ‡πÅ‡∏•‡∏∞ logger
        """
        self.db_config = DatabaseConfig()
        self.engine = self.db_config.get_engine()
        self.logger = logging.getLogger(__name__)
        self.permission_checker = None  # ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≠‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

    def _get_permission_checker(self, log_callback=None):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ PermissionCheckerService"""
        if self.permission_checker is None:
            # ‡πÉ‡∏ä‡πâ silent callback ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GUI
            default_callback = log_callback or (lambda msg: None)
            self.permission_checker = PermissionCheckerService(
                engine=self.engine, 
                log_callback=default_callback
            )
        return self.permission_checker

    def check_permissions(self, schema_name: str = 'bronze', log_callback=None) -> Dict:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå SQL Server ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
        
        Args:
            schema_name: ‡∏ä‡∏∑‡πà‡∏≠ schema ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
            log_callback: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á log (None = ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á log)
            
        Returns:
            Dict: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå
        """
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GUI ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á log ‡πÉ‡∏ô CLI
        silent_callback = lambda msg: None  # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏á‡∏µ‡∏¢‡∏ö
        checker = self._get_permission_checker(silent_callback if log_callback is None else log_callback)
        return checker.check_all_permissions(schema_name)

    def generate_permission_report(self, schema_name: str = 'bronze') -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
        
        Args:
            schema_name: ‡∏ä‡∏∑‡πà‡∏≠ schema ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
            
        Returns:
            str: ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå
        """
        checker = self._get_permission_checker()
        return checker.generate_permission_report(schema_name)

    def check_connection(self, show_warning: bool = True) -> Tuple[bool, str]:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö SQL Server
        
        Returns:
            Tuple[bool, str]: (‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠, ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå)
        """
        try:
            with self.engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            return True, SuccessMessages.DB_CONNECTION_SUCCESS
        except Exception as e:
            error_msg = f"{ErrorMessages.DB_CONNECTION_FAILED}: {e}"
            self.logger.error(error_msg)
            if show_warning:
                messagebox.showwarning("‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", error_msg)
            return False, error_msg

    def test_connection(self, config: Dict[str, Any]) -> bool:
        """
        ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö SQL Server ‡∏î‡πâ‡∏ß‡∏¢ config ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏°‡∏≤
        
        Args:
            config (Dict[str, Any]): ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
            
        Returns:
            bool: True ‡∏´‡∏≤‡∏Å‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à, False ‡∏´‡∏≤‡∏Å‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß
        """
        try:
            driver = DatabaseConstants.DEFAULT_DRIVER
            if config["auth_type"] == DatabaseConstants.AUTH_WINDOWS:
                conn_str = (
                    f"mssql+pyodbc://@{config['server']}/{config['database']}?"
                    f"driver={driver}&Trusted_Connection=yes"
                )
            else:
                conn_str = (
                    f"mssql+pyodbc://{config['username']}:{config['password']}@{config['server']}/{config['database']}?"
                    f"driver={driver}"
                )
            engine = create_engine(conn_str)
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            return True
        except Exception as e:
            self.logger.error(f"Connection error: {e}")
            return False

    def update_config(self, server=None, database=None, auth_type=None, username=None, password=None):
        """‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠"""
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô DatabaseConfig
        self.db_config.update_config(
            server=server,
            database=database,
            auth_type=auth_type,
            username=username,
            password=password
        )
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï engine
        self.engine = self.db_config.get_engine()

    def ensure_schemas_exist(self, schema_names):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á schema ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏ ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ"""
        try:
            with self.engine.begin() as conn:
                for schema_name in schema_names:
                    conn.execute(text(f"""
                        IF NOT EXISTS (
                            SELECT 1 FROM sys.schemas WHERE name = '{schema_name}'
                        )
                        BEGIN
                            EXEC('CREATE SCHEMA {schema_name}')
                        END
                    """))
            return True, f"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö/‡∏™‡∏£‡πâ‡∏≤‡∏á schema ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢: {', '.join(schema_names)}"
        except Exception as e:
            error_msg = f"‡∏™‡∏£‡πâ‡∏≤‡∏á schema ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}"
            self.logger.error(error_msg)
            messagebox.showwarning("‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Schema", error_msg)
            return False, error_msg

    def _fix_text_columns_to_nvarchar_max(self, table_name, required_cols, schema_name='bronze', log_func=None):
        """‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Text() ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô NVARCHAR(MAX) ‡πÉ‡∏ô SQL Server"""
        from sqlalchemy.types import Text
        
        try:
            with self.engine.begin() as conn:
                for col_name, dtype in required_cols.items():
                    if isinstance(dtype, Text):
                        alter_sql = f"ALTER TABLE {schema_name}.{table_name} ALTER COLUMN [{col_name}] NVARCHAR(MAX)"
                        if log_func:
                            log_func(f"üîß ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '{col_name}' ‡πÄ‡∏õ‡πá‡∏ô NVARCHAR(MAX)")
                        conn.execute(text(alter_sql))
        except Exception as e:
            if log_func:
                log_func(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Text() ‡πÑ‡∏î‡πâ: {e}")

    def upload_data(self, df, logic_type, required_cols, schema_name='bronze', log_func=None, force_recreate=False):
        """
        ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏ï‡∏≤‡∏° config, insert ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ, ‡∏ñ‡πâ‡∏≤ schema DB ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡πÉ‡∏´‡πâ drop ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
        
        Args:
            df: DataFrame ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î
            logic_type: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
            required_cols: ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏•‡∏∞‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            schema_name: ‡∏ä‡∏∑‡πà‡∏≠ schema ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            log_func: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö log
            force_recreate: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà (‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)
        """
        
        # ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏ú‡πà‡∏≤‡∏ô ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
        if log_func:
            log_func("‚úÖ ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
        
        try:
            import json
            from datetime import datetime
            from sqlalchemy.types import DateTime
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
            if df is None or df.empty:
                return False, "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤"
            
            if not required_cols:
                return False, "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå timestamp
            current_time = datetime.now()
            df['updated_at'] = current_time
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° dtype ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå timestamp
            required_cols['updated_at'] = DateTime()
            
            table_name = None
            try:
                with open('config/column_settings.json', 'r', encoding='utf-8') as f:
                    col_config = json.load(f)
                table_name = col_config.get("__table_names__", {}).get(logic_type)
            except Exception:
                table_name = None
            if not table_name:
                table_name = logic_type

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á schema ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
            schema_result = self.ensure_schemas_exist([schema_name])
            if not schema_result[0]:
                return False, f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á schema ‡πÑ‡∏î‡πâ: {schema_result[1]}"

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö schema DB ‡∏ß‡πà‡∏≤‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö required_cols ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            from sqlalchemy import inspect
            from sqlalchemy.types import Text
            insp = inspect(self.engine)
            needs_recreate = force_recreate  # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏ñ‡πâ‡∏≤ auto-fix ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
            
            if insp.has_table(table_name, schema=schema_name) and not force_recreate:
                db_cols = [col['name'] for col in insp.get_columns(table_name, schema=schema_name)]
                db_col_types = {col['name']: str(col['type']).upper() for col in insp.get_columns(table_name, schema=schema_name)}
                config_cols = list(required_cols.keys())
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
                if set(db_cols) != set(config_cols):
                    msg = f"‚ùå ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡∏≠‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á {schema_name}.{table_name} ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö config"
                    needs_recreate = True
                else:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö data types ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
                    from sqlalchemy.types import NVARCHAR as SA_NVARCHAR, DateTime as SA_DateTime, DATE as SA_DATE
                    from sqlalchemy.types import Integer as SA_Integer, SmallInteger as SA_SmallInteger, Float as SA_Float, DECIMAL as SA_DECIMAL, Text as SA_Text, Boolean as SA_Boolean

                    def _type_category(type_str: str) -> str:
                        t = (type_str or '').upper()
                        if any(x in t for x in ['INT', 'BIGINT', 'SMALLINT', 'FLOAT', 'DECIMAL', 'NUMERIC', 'REAL', 'MONEY']):
                            return 'NUMERIC'
                        if any(x in t for x in ['DATE', 'DATETIME', 'SMALLDATETIME', 'DATETIME2', 'TIME']):
                            return 'DATETIME'
                        if any(x in t for x in ['BIT', 'BOOLEAN']):
                            return 'BOOLEAN'
                        if any(x in t for x in ['CHAR', 'NCHAR', 'VARCHAR', 'NVARCHAR', 'TEXT']):
                            return 'STRING'
                        return 'OTHER'

                    def _expected_type_str(sa_type_obj) -> str:
                        # ‡πÅ‡∏õ‡∏•‡∏á SQLAlchemy type object ‡πÄ‡∏õ‡πá‡∏ô string ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö DB
                        s = str(sa_type_obj).upper()
                        # Map Text() ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏õ‡πá‡∏ô NVARCHAR(MAX)
                        if isinstance(sa_type_obj, SA_Text):
                            return 'NVARCHAR(MAX)'
                        return s

                    def _parse_varchar_len(type_str: str) -> int:
                        try:
                            if 'MAX' in type_str.upper():
                                return 2_147_483_647  # ‡πÉ‡∏´‡∏ç‡πà‡∏û‡∏≠‡πÅ‡∏ó‡∏ô MAX
                            if '(' in type_str and ')' in type_str:
                                return int(type_str.split('(')[1].split(')')[0])
                        except Exception:
                            pass
                        return -1

                    for col_name, expected_dtype in required_cols.items():
                        db_type = db_col_types.get(col_name, '')
                        expected_str = _expected_type_str(expected_dtype)
                        cat_db = _type_category(db_type)
                        cat_expected = _type_category(expected_str)

                        # ‡∏Å‡∏£‡∏ì‡∏µ Text() ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô NVARCHAR(MAX)
                        if isinstance(expected_dtype, SA_Text):
                            if 'NVARCHAR(MAX)' not in db_type and 'TEXT' not in db_type:
                                if log_func:
                                    log_func(f"‚ùå ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '{col_name}' ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô NVARCHAR(MAX) ‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡πá‡∏ô {db_type}")
                                needs_recreate = True
                                break

                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö mismatch ‡∏Ç‡∏≠‡∏á‡∏´‡∏°‡∏ß‡∏î‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÄ‡∏ä‡πà‡∏ô STRING ‚Üî NUMERIC)
                        if cat_db != cat_expected:
                            if log_func:
                                log_func(f"‚ùå ‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '{col_name}' ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô (DB: {db_type} | Expected: {expected_str})")
                            needs_recreate = True
                            break

                        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô STRING ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß NVARCHAR
                        if cat_expected == 'STRING' and 'NVARCHAR' in expected_str:
                            exp_len = _parse_varchar_len(expected_str)
                            act_len = _parse_varchar_len(db_type)
                            if act_len != -1 and exp_len != -1 and act_len < exp_len:
                                if log_func:
                                    log_func(f"‚ùå ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß NVARCHAR ‡∏Ç‡∏≠‡∏á '{col_name}' ‡πÑ‡∏°‡πà‡∏û‡∏≠ (DB: {db_type} | Expected: {expected_str})")
                                needs_recreate = True
                                break
            
            if needs_recreate or not insp.has_table(table_name, schema=schema_name):
                if force_recreate and log_func:
                    log_func(f"üîÑ ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á {schema_name}.{table_name} ‡πÉ‡∏´‡∏°‡πà")
                elif needs_recreate and log_func:
                    log_func(f"‚ùå Schema ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á {schema_name}.{table_name} ‡πÉ‡∏´‡∏°‡πà")
                elif log_func:
                    log_func(f"üìã ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà {schema_name}.{table_name}")
                    
                # Drop ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
                df.head(0)[list(required_cols.keys())].to_sql(
                    name=table_name,
                    con=self.engine,
                    schema=schema_name,
                    if_exists='replace',
                    index=False,
                    dtype=required_cols
                )
                # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Text() ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô NVARCHAR(MAX)
                self._fix_text_columns_to_nvarchar_max(table_name, required_cols, schema_name, log_func)
            else:
                # ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°
                if log_func:
                    log_func(f"üóëÔ∏è ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á {schema_name}.{table_name}")
                with self.engine.begin() as conn:
                    conn.execute(text(f"TRUNCATE TABLE {schema_name}.{table_name}"))
            
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: ‡∏Ñ‡∏á‡πÄ‡∏õ‡πá‡∏ô dtype datetime ‡∏Ç‡∏≠‡∏á pandas ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ SQLAlchemy ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ ‡πÑ‡∏°‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏ï‡∏£‡∏¥‡∏á
            for col, dtype in required_cols.items():
                dtype_str = str(dtype).lower()
                if col in df.columns and ("date" in dtype_str or "datetime" in dtype_str):
                    try:
                        # re-parse ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà datetime dtype
                        import pandas as pd
                        from pandas.api.types import is_datetime64_any_dtype
                        if not is_datetime64_any_dtype(df[col]):
                            df[col] = pd.to_datetime(df[col], errors="coerce")
                        # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà SQL Server ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö
                        min_sql_date = pd.Timestamp('1753-01-01')
                        max_sql_date = pd.Timestamp('9999-12-31 23:59:59')
                        valid_mask = df[col].notna() & (df[col] >= min_sql_date) & (df[col] <= max_sql_date)
                        df.loc[~valid_mask, col] = pd.NaT
                    except Exception:
                        # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° (to_sql ‡∏à‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏° dtype)
                        pass
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á
            if df.empty:
                return False, "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á"
            
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏ä‡πâ bcpandas ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß (fallback ‡πÑ‡∏õ to_sql ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ)
            try:
                from bcpandas import SqlCreds, to_sql as bcp_to_sql  # type: ignore

                cfg = getattr(self.db_config, 'config', {}) or {}
                if cfg.get('auth_type') == DatabaseConstants.AUTH_WINDOWS:
                    creds = SqlCreds(
                        server=cfg.get('server', ''),
                        database=cfg.get('database', ''),
                        trusted_connection='yes'
                    )
                else:
                    creds = SqlCreds(
                        server=cfg.get('server', ''),
                        database=cfg.get('database', ''),
                        username=cfg.get('username', ''),
                        password=cfg.get('password', '')
                    )

                data_to_load = df[list(required_cols.keys())]
                if log_func:
                    log_func(f"‚ö° ‡πÉ‡∏ä‡πâ bcp ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß: {len(data_to_load):,} ‡πÅ‡∏ñ‡∏ß ‚Üí {schema_name}.{table_name}")

                # ‡πÉ‡∏ä‡πâ bcp ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö append (‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô)
                bcp_to_sql(
                    data_to_load,
                    table_name,
                    creds,
                    index=False,
                    schema=schema_name,
                    if_exists='append'
                )
                return True, f" {schema_name}.{table_name} ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(data_to_load):,} ‡πÅ‡∏ñ‡∏ß‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (bcp)"
            except Exception as be:
                if log_func:
                    log_func(f"‚ö†Ô∏è bcp ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô/‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏õ‡∏Å‡∏ï‡∏¥‡πÅ‡∏ó‡∏ô: {be}")

                # Fallback ‡πÄ‡∏õ‡πá‡∏ô pandas.to_sql ‡πÅ‡∏ö‡∏ö chunked ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà
                if len(df) > 10000:
                    if log_func:
                        log_func(f"üìä ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà ({len(df):,} ‡πÅ‡∏ñ‡∏ß) - ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö chunked (fallback)")
                    chunk_size = 5000
                    total_chunks = (len(df) + chunk_size - 1) // chunk_size
                    uploaded_rows = 0
                    for i in range(0, len(df), chunk_size):
                        chunk = df.iloc[i:i+chunk_size]
                        chunk[list(required_cols.keys())].to_sql(
                            name=table_name,
                            con=self.engine,
                            schema=schema_name,
                            if_exists='append',
                            index=False,
                            dtype=required_cols
                        )
                        uploaded_rows += len(chunk)
                        chunk_num = (i // chunk_size) + 1
                        if log_func:
                            log_func(f"üì§ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î chunk {chunk_num}/{total_chunks}: {len(chunk):,} ‡πÅ‡∏ñ‡∏ß")
                    return True, f" {schema_name}.{table_name} ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {uploaded_rows:,} ‡πÅ‡∏ñ‡∏ß‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (fallback chunked)"
                else:
                    df[list(required_cols.keys())].to_sql(
                        name=table_name,
                        con=self.engine,
                        schema=schema_name,
                        if_exists='append',
                        index=False,
                        dtype=required_cols
                    )
                    return True, f" {schema_name}.{table_name} ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {df.shape[0]} ‡πÅ‡∏ñ‡∏ß‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (fallback)"
        except Exception as e:
            # ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏ä‡∏µ‡πâ‡πÄ‡∏õ‡πâ‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
            def _short_exception_message(exc: Exception) -> str:
                try:
                    if isinstance(exc, DBAPIError) and getattr(exc, "orig", None):
                        # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏à‡∏≤‡∏Å DBAPI/pyodbc ‡πÇ‡∏î‡∏¢‡∏ï‡∏±‡∏î SQL ‡πÅ‡∏•‡∏∞ parameters ‡∏ó‡∏¥‡πâ‡∏á
                        orig = exc.orig
                        # pyodbc ‡∏à‡∏∞‡∏°‡∏µ args ‡πÄ‡∏õ‡πá‡∏ô tuple ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß ‡πÄ‡∏£‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏î
                        parts = [str(p) for p in getattr(orig, "args", []) if p]
                        core = parts[-1] if parts else str(exc)
                    else:
                        core = str(exc)
                    # ‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô [SQL: ...] ‡πÅ‡∏•‡∏∞ [parameters: ...] ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
                    for token in ["[SQL:", "[parameters:"]:
                        if token in core:
                            core = core.split(token)[0].strip()
                    # ‡∏¢‡πà‡∏≠‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô (‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å)
                    core = core.splitlines()[0]
                    return core
                except Exception:
                    return str(exc)

            def _detect_problem_columns(df_local, required_map, max_cols: int = 5, max_examples: int = 3):
                import pandas as pd  # ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á import ‡∏ß‡∏ô
                from sqlalchemy.types import (
                    Integer as SA_Integer,
                    SmallInteger as SA_SmallInteger,
                    Float as SA_Float,
                    DECIMAL as SA_DECIMAL,
                    DATE as SA_DATE,
                    DateTime as SA_DateTime,
                    NVARCHAR as SA_NVARCHAR,
                )
                problems = []
                try:
                    for col, dtype in required_map.items():
                        if col not in df_local.columns:
                            continue
                        # ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
                        if isinstance(dtype, (SA_Integer, SA_SmallInteger, SA_Float, SA_DECIMAL)):
                            numeric_series = pd.to_numeric(df_local[col], errors="coerce")
                            mask = numeric_series.isna() & df_local[col].notna()
                            bad_count = int(mask.sum())
                            if bad_count > 0:
                                examples = [str(x) for x in df_local.loc[mask, col].head(max_examples).tolist()]
                                problems.append({
                                    "column": col,
                                    "expected": str(dtype),
                                    "bad_count": bad_count,
                                    "examples": examples,
                                })
                        # ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
                        elif isinstance(dtype, (SA_DATE, SA_DateTime)):
                            parsed = pd.to_datetime(df_local[col], errors="coerce")
                            mask = parsed.isna() & df_local[col].notna()
                            bad_count = int(mask.sum())
                            if bad_count > 0:
                                examples = [str(x) for x in df_local.loc[mask, col].head(max_examples).tolist()]
                                problems.append({
                                    "column": col,
                                    "expected": str(dtype),
                                    "bad_count": bad_count,
                                    "examples": examples,
                                })
                        # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á string (‡∏ñ‡πâ‡∏≤‡∏Å‡∏≥‡∏´‡∏ô‡∏î NVARCHAR(n))
                        elif isinstance(dtype, SA_NVARCHAR) and getattr(dtype, "length", None):
                            max_len = int(dtype.length)
                            str_series = df_local[col].astype(str)
                            mask = str_series.str.len() > max_len
                            bad_count = int(mask.sum())
                            if bad_count > 0:
                                examples = str_series.loc[mask].str[:50].head(max_examples).tolist()
                                problems.append({
                                    "column": col,
                                    "expected": f"NVARCHAR({max_len})",
                                    "bad_count": bad_count,
                                    "examples": examples,
                                })
                        # ‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ
                        if len(problems) >= max_cols:
                            break
                except Exception:
                    pass
                return problems

            short_msg = _short_exception_message(e)
            problem_hints = _detect_problem_columns(df, required_cols)

            if problem_hints:
                lines = [
                    f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {short_msg}",
                    "‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (‡πÅ‡∏™‡∏î‡∏á‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô):",
                ]
                for p in problem_hints:
                    ex = ", ".join(p.get("examples", []))
                    lines.append(f"- {p['column']} (‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤ {p['expected']}) ‡∏ú‡∏¥‡∏î {p['bad_count']:,} ‡πÅ‡∏ñ‡∏ß ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: [{ex}]")
                error_msg = "\n".join(lines)
            else:
                error_msg = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {short_msg}"

            if log_func:
                log_func(f"‚ùå {error_msg}")
            return False, error_msg
