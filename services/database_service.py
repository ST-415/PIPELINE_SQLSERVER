"""
Database Service ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PIPELINE_SQLSERVER

‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á SQL Server
"""

import logging
import os
from tkinter import messagebox
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
from sqlalchemy import create_engine, text

from config.database import DatabaseConfig
from constants import DatabaseConstants, ErrorMessages, SuccessMessages
from .permission_checker_service import PermissionCheckerService

class DatabaseService:
    """
    ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• SQL Server
    
    ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ schema
    """
    
    def __init__(self) -> None:
        """
        ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô DatabaseService
        
        ‡∏™‡∏£‡πâ‡∏≤‡∏á database config, engine ‡πÅ‡∏•‡∏∞ logger
        """
        self.db_config = DatabaseConfig()
        self.engine = self.db_config.get_engine()
        self.logger = logging.getLogger(__name__)
        self.permission_checker = None  # ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≠‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

    def _get_permission_checker(self, log_callback=None):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ PermissionCheckerService"""
        if self.permission_checker is None:
            # ‡πÉ‡∏ä‡πâ silent callback ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GUI
            default_callback = log_callback or (lambda msg: None)
            self.permission_checker = PermissionCheckerService(
                engine=self.engine, 
                log_callback=default_callback
            )
        return self.permission_checker

    def check_permissions(self, schema_name: str = 'bronze', log_callback=None) -> Dict:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå SQL Server ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
        
        Args:
            schema_name: ‡∏ä‡∏∑‡πà‡∏≠ schema ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
            log_callback: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á log (None = ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á log)
            
        Returns:
            Dict: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå
        """
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GUI ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á log ‡πÉ‡∏ô CLI
        silent_callback = lambda msg: None  # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏á‡∏µ‡∏¢‡∏ö
        checker = self._get_permission_checker(silent_callback if log_callback is None else log_callback)
        return checker.check_all_permissions(schema_name)

    def generate_permission_report(self, schema_name: str = 'bronze') -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
        
        Args:
            schema_name: ‡∏ä‡∏∑‡πà‡∏≠ schema ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
            
        Returns:
            str: ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå
        """
        checker = self._get_permission_checker()
        return checker.generate_permission_report(schema_name)

    def check_connection(self, show_warning: bool = True) -> Tuple[bool, str]:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö SQL Server
        
        Returns:
            Tuple[bool, str]: (‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠, ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå)
        """
        try:
            with self.engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            return True, SuccessMessages.DB_CONNECTION_SUCCESS
        except Exception as e:
            error_msg = f"{ErrorMessages.DB_CONNECTION_FAILED}: {e}"
            self.logger.error(error_msg)
            if show_warning:
                messagebox.showwarning("‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", error_msg)
            return False, error_msg

    def test_connection(self, config: Dict[str, Any]) -> bool:
        """
        ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö SQL Server ‡∏î‡πâ‡∏ß‡∏¢ config ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏°‡∏≤
        
        Args:
            config (Dict[str, Any]): ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
            
        Returns:
            bool: True ‡∏´‡∏≤‡∏Å‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à, False ‡∏´‡∏≤‡∏Å‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß
        """
        try:
            driver = DatabaseConstants.DEFAULT_DRIVER
            if config["auth_type"] == DatabaseConstants.AUTH_WINDOWS:
                conn_str = (
                    f"mssql+pyodbc://@{config['server']}/{config['database']}?"
                    f"driver={driver}&Trusted_Connection=yes"
                )
            else:
                conn_str = (
                    f"mssql+pyodbc://{config['username']}:{config['password']}@{config['server']}/{config['database']}?"
                    f"driver={driver}"
                )
            engine = create_engine(conn_str)
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            return True
        except Exception as e:
            self.logger.error(f"Connection error: {e}")
            return False

    def update_config(self, server=None, database=None, auth_type=None, username=None, password=None):
        """‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠"""
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô DatabaseConfig
        self.db_config.update_config(
            server=server,
            database=database,
            auth_type=auth_type,
            username=username,
            password=password
        )
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï engine
        self.engine = self.db_config.get_engine()

    def ensure_schemas_exist(self, schema_names):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á schema ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏ ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ"""
        try:
            with self.engine.begin() as conn:
                for schema_name in schema_names:
                    conn.execute(text(f"""
                        IF NOT EXISTS (
                            SELECT 1 FROM sys.schemas WHERE name = '{schema_name}'
                        )
                        BEGIN
                            EXEC('CREATE SCHEMA {schema_name}')
                        END
                    """))
            return True, f"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö/‡∏™‡∏£‡πâ‡∏≤‡∏á schema ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢: {', '.join(schema_names)}"
        except Exception as e:
            error_msg = f"‡∏™‡∏£‡πâ‡∏≤‡∏á schema ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}"
            self.logger.error(error_msg)
            messagebox.showwarning("‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Schema", error_msg)
            return False, error_msg

    def _fix_text_columns_to_nvarchar_max(self, table_name, required_cols, schema_name='bronze', log_func=None):
        """‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Text() ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô NVARCHAR(MAX) ‡πÉ‡∏ô SQL Server"""
        from sqlalchemy.types import Text
        
        try:
            with self.engine.begin() as conn:
                for col_name, dtype in required_cols.items():
                    if isinstance(dtype, Text):
                        alter_sql = f"ALTER TABLE {schema_name}.{table_name} ALTER COLUMN [{col_name}] NVARCHAR(MAX)"
                        if log_func:
                            log_func(f"üîß ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '{col_name}' ‡πÄ‡∏õ‡πá‡∏ô NVARCHAR(MAX)")
                        conn.execute(text(alter_sql))
        except Exception as e:
            if log_func:
                log_func(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Text() ‡πÑ‡∏î‡πâ: {e}")

    def upload_data(self, df, logic_type, required_cols, schema_name='bronze', log_func=None, force_recreate=False):
        """
        ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏ï‡∏≤‡∏° config, insert ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ, ‡∏ñ‡πâ‡∏≤ schema DB ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡πÉ‡∏´‡πâ drop ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
        
        Args:
            df: DataFrame ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î
            logic_type: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
            required_cols: ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏•‡∏∞‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            schema_name: ‡∏ä‡∏∑‡πà‡∏≠ schema ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            log_func: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö log
            force_recreate: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà (‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)
        """
        
        # ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏ú‡πà‡∏≤‡∏ô ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
        if log_func:
            log_func("‚úÖ ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
        
        try:
            import json
            from datetime import datetime
            from sqlalchemy.types import DateTime
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
            if df is None or df.empty:
                return False, "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤"
            
            if not required_cols:
                return False, "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå timestamp
            current_time = datetime.now()
            df['updated_at'] = current_time
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° dtype ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå timestamp
            required_cols['updated_at'] = DateTime()
            
            table_name = None
            try:
                with open('config/column_settings.json', 'r', encoding='utf-8') as f:
                    col_config = json.load(f)
                table_name = col_config.get("__table_names__", {}).get(logic_type)
            except Exception:
                table_name = None
            if not table_name:
                table_name = logic_type

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á schema ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
            schema_result = self.ensure_schemas_exist([schema_name])
            if not schema_result[0]:
                return False, f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á schema ‡πÑ‡∏î‡πâ: {schema_result[1]}"

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö schema DB ‡∏ß‡πà‡∏≤‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö required_cols ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            from sqlalchemy import inspect
            from sqlalchemy.types import Text
            insp = inspect(self.engine)
            needs_recreate = force_recreate  # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏ñ‡πâ‡∏≤ auto-fix ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
            
            if insp.has_table(table_name, schema=schema_name) and not force_recreate:
                db_cols = [col['name'] for col in insp.get_columns(table_name, schema=schema_name)]
                db_col_types = {col['name']: str(col['type']).upper() for col in insp.get_columns(table_name, schema=schema_name)}
                config_cols = list(required_cols.keys())
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
                if set(db_cols) != set(config_cols):
                    msg = f"‚ùå ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡∏≠‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á {schema_name}.{table_name} ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö config"
                    needs_recreate = True
                else:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö data types ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
                    from sqlalchemy.types import NVARCHAR as SA_NVARCHAR, DateTime as SA_DateTime, DATE as SA_DATE
                    from sqlalchemy.types import Integer as SA_Integer, SmallInteger as SA_SmallInteger, Float as SA_Float, DECIMAL as SA_DECIMAL, Text as SA_Text, Boolean as SA_Boolean

                    def _type_category(type_str: str) -> str:
                        t = (type_str or '').upper()
                        if any(x in t for x in ['INT', 'BIGINT', 'SMALLINT', 'FLOAT', 'DECIMAL', 'NUMERIC', 'REAL', 'MONEY']):
                            return 'NUMERIC'
                        if any(x in t for x in ['DATE', 'DATETIME', 'SMALLDATETIME', 'DATETIME2', 'TIME']):
                            return 'DATETIME'
                        if any(x in t for x in ['BIT', 'BOOLEAN']):
                            return 'BOOLEAN'
                        if any(x in t for x in ['CHAR', 'NCHAR', 'VARCHAR', 'NVARCHAR', 'TEXT']):
                            return 'STRING'
                        return 'OTHER'

                    def _expected_type_str(sa_type_obj) -> str:
                        # ‡πÅ‡∏õ‡∏•‡∏á SQLAlchemy type object ‡πÄ‡∏õ‡πá‡∏ô string ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö DB
                        s = str(sa_type_obj).upper()
                        # Map Text() ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏õ‡πá‡∏ô NVARCHAR(MAX)
                        if isinstance(sa_type_obj, SA_Text):
                            return 'NVARCHAR(MAX)'
                        return s

                    def _parse_varchar_len(type_str: str) -> int:
                        try:
                            if 'MAX' in type_str.upper():
                                return 2_147_483_647  # ‡πÉ‡∏´‡∏ç‡πà‡∏û‡∏≠‡πÅ‡∏ó‡∏ô MAX
                            if '(' in type_str and ')' in type_str:
                                return int(type_str.split('(')[1].split(')')[0])
                        except Exception:
                            pass
                        return -1

                    for col_name, expected_dtype in required_cols.items():
                        db_type = db_col_types.get(col_name, '')
                        expected_str = _expected_type_str(expected_dtype)
                        cat_db = _type_category(db_type)
                        cat_expected = _type_category(expected_str)

                        # ‡∏Å‡∏£‡∏ì‡∏µ Text() ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô NVARCHAR(MAX)
                        if isinstance(expected_dtype, SA_Text):
                            if 'NVARCHAR(MAX)' not in db_type and 'TEXT' not in db_type:
                                if log_func:
                                    log_func(f"‚ùå ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '{col_name}' ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô NVARCHAR(MAX) ‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡πá‡∏ô {db_type}")
                                needs_recreate = True
                                break

                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö mismatch ‡∏Ç‡∏≠‡∏á‡∏´‡∏°‡∏ß‡∏î‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÄ‡∏ä‡πà‡∏ô STRING ‚Üî NUMERIC)
                        if cat_db != cat_expected:
                            if log_func:
                                log_func(f"‚ùå ‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '{col_name}' ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô (DB: {db_type} | Expected: {expected_str})")
                            needs_recreate = True
                            break

                        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô STRING ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß NVARCHAR
                        if cat_expected == 'STRING' and 'NVARCHAR' in expected_str:
                            exp_len = _parse_varchar_len(expected_str)
                            act_len = _parse_varchar_len(db_type)
                            if act_len != -1 and exp_len != -1 and act_len < exp_len:
                                if log_func:
                                    log_func(f"‚ùå ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß NVARCHAR ‡∏Ç‡∏≠‡∏á '{col_name}' ‡πÑ‡∏°‡πà‡∏û‡∏≠ (DB: {db_type} | Expected: {expected_str})")
                                needs_recreate = True
                                break
            
            if needs_recreate or not insp.has_table(table_name, schema=schema_name):
                if force_recreate and log_func:
                    log_func(f"üîÑ ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á {schema_name}.{table_name} ‡πÉ‡∏´‡∏°‡πà")
                elif needs_recreate and log_func:
                    log_func(f"‚ùå Schema ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á {schema_name}.{table_name} ‡πÉ‡∏´‡∏°‡πà")
                elif log_func:
                    log_func(f"üìã ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà {schema_name}.{table_name}")
                    
                # Drop ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
                df.head(0)[list(required_cols.keys())].to_sql(
                    name=table_name,
                    con=self.engine,
                    schema=schema_name,
                    if_exists='replace',
                    index=False,
                    dtype=required_cols
                )
                # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Text() ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô NVARCHAR(MAX)
                self._fix_text_columns_to_nvarchar_max(table_name, required_cols, schema_name, log_func)
            else:
                # ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°
                if log_func:
                    log_func(f"üóëÔ∏è ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á {schema_name}.{table_name}")
                with self.engine.begin() as conn:
                    conn.execute(text(f"TRUNCATE TABLE {schema_name}.{table_name}"))
            
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: ‡∏Ñ‡∏á‡πÄ‡∏õ‡πá‡∏ô dtype datetime ‡∏Ç‡∏≠‡∏á pandas ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ SQLAlchemy ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ ‡πÑ‡∏°‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏ï‡∏£‡∏¥‡∏á
            for col, dtype in required_cols.items():
                dtype_str = str(dtype).lower()
                if col in df.columns and ("date" in dtype_str or "datetime" in dtype_str):
                    try:
                        # re-parse ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà datetime dtype
                        import pandas as pd
                        from pandas.api.types import is_datetime64_any_dtype
                        if not is_datetime64_any_dtype(df[col]):
                            df[col] = pd.to_datetime(df[col], errors="coerce")
                        # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà SQL Server ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö
                        min_sql_date = pd.Timestamp('1753-01-01')
                        max_sql_date = pd.Timestamp('9999-12-31 23:59:59')
                        valid_mask = df[col].notna() & (df[col] >= min_sql_date) & (df[col] <= max_sql_date)
                        df.loc[~valid_mask, col] = pd.NaT
                    except Exception:
                        # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° (to_sql ‡∏à‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏° dtype)
                        pass
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á
            if df.empty:
                return False, "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á"
            
            # ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö chunked ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà
            if len(df) > 10000:  # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤ 10,000 ‡πÅ‡∏ñ‡∏ß
                if log_func:
                    log_func(f"üìä ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà ({len(df):,} ‡πÅ‡∏ñ‡∏ß) - ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö chunked")
                
                chunk_size = 5000
                total_chunks = (len(df) + chunk_size - 1) // chunk_size
                uploaded_rows = 0
                
                for i in range(0, len(df), chunk_size):
                    chunk = df.iloc[i:i+chunk_size]
                    chunk[list(required_cols.keys())].to_sql(
                        name=table_name,
                        con=self.engine,
                        schema=schema_name,
                        if_exists='append',
                        index=False,
                        dtype=required_cols
                    )
                    uploaded_rows += len(chunk)
                    
                    chunk_num = (i // chunk_size) + 1
                    if log_func:
                        log_func(f"üì§ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î chunk {chunk_num}/{total_chunks}: {len(chunk):,} ‡πÅ‡∏ñ‡∏ß")
                
                return True, f" {schema_name}.{table_name} ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {uploaded_rows:,} ‡πÅ‡∏ñ‡∏ß‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡πÅ‡∏ö‡∏ö chunked)"
            else:
                # ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏•‡πá‡∏Å
                df[list(required_cols.keys())].to_sql(
                    name=table_name,
                    con=self.engine,
                    schema=schema_name,
                    if_exists='append',
                    index=False,
                    dtype=required_cols
                )
                return True, f" {schema_name}.{table_name} ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {df.shape[0]} ‡πÅ‡∏ñ‡∏ß‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à"
        except Exception as e:
            error_msg = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}"
            if log_func:
                log_func(f"‚ùå {error_msg}")
            return False, error_msg
