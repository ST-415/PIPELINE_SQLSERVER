"""
File Service ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PIPELINE_SQLSERVER (‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡πÅ‡∏•‡πâ‡∏ß)

‡πÄ‡∏õ‡πá‡∏ô orchestrator ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° services ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô:
- FileReaderService: ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå
- DataProcessorService: ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
- FileManagementService: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:
    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GUI
    file_service = FileService(log_callback=gui_log_function)
    
    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CLI
    file_service = FileService(log_callback=logging.info)
    
    # ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥ (interface ‡πÄ‡∏î‡∏¥‡∏°)
    success, df = file_service.read_excel_file("data.xlsx", "sales_data")
    
    # ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô
    file_info = file_service.get_file_info("data.xlsx")
    validation = file_service.validate_file_before_processing("data.xlsx", "sales_data")
"""

from typing import Optional, Tuple
import logging

from .file import (
    FileReaderService,
    DataProcessorService,
    FileManagementService
)
from performance_optimizations import PerformanceOptimizer
from config.settings import settings_manager


class FileService:
    """
    ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å (orchestrator)
    
    ‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö:
    - ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ô‡∏á‡∏≤‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á services ‡∏ï‡πà‡∏≤‡∏á‡πÜ
    - ‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£
    - ‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ interface ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°
    """
    
    def __init__(self, search_path: Optional[str] = None, log_callback: Optional[callable] = None) -> None:
        """
        ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô FileService
        
        Args:
            search_path (Optional[str]): ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå
            log_callback (Optional[callable]): ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á log
        """
        self.log_callback = log_callback if log_callback else logging.info
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á services
        self.file_reader = FileReaderService(search_path, self.log_callback)
        self.data_processor = DataProcessorService(self.log_callback)
        self.file_manager = FileManagementService(search_path)
        
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å SettingsManager
        self.file_reader.column_settings = settings_manager.column_settings
        self.file_reader.dtype_settings = settings_manager.dtype_settings
        self.file_reader._settings_loaded = True
        
        self.data_processor.column_settings = settings_manager.column_settings  
        self.data_processor.dtype_settings = settings_manager.dtype_settings
        self.data_processor._settings_loaded = True
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á performance optimizer
        self.performance_optimizer = PerformanceOptimizer(self.log_callback)
        
        # ‡πÄ‡∏Å‡πá‡∏ö reference ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö backward compatibility
        self.search_path = self.file_reader.search_path
        self.column_settings = settings_manager.column_settings

    # ========================
    # Main Interface Methods
    # ========================
    
    def preview_file_columns(self, file_path, logic_type, max_rows=5):
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏î‡∏¢‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤
        
        Args:
            file_path: ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå
            logic_type: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
            max_rows: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö preview (default: 5)
            
        Returns:
            tuple: (success, result/error_message, columns_info)
        """
        try:
            import pandas as pd
            
            # ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
            if file_path.lower().endswith('.csv'):
                try:
                    preview_df = pd.read_csv(file_path, nrows=max_rows, encoding='utf-8')
                except UnicodeDecodeError:
                    preview_df = pd.read_csv(file_path, nrows=max_rows, encoding='tis-620')
            else:
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Excel
                preview_df = pd.read_excel(file_path, nrows=max_rows)
            
            if preview_df.empty:
                return False, "File is empty", None
            
            # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏≤‡∏Å preview
            file_columns = list(preview_df.columns)
            
            # Apply column mapping ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏∞‡∏ñ‡∏π‡∏Å rename ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
            col_map = self.file_reader.build_rename_mapping_for_dataframe(file_columns, logic_type)
            
            mapped_columns = file_columns.copy()
            if col_map:
                for old_name, new_name in col_map.items():
                    if old_name in mapped_columns:
                        idx = mapped_columns.index(old_name)
                        mapped_columns[idx] = new_name
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
            success, validation_message = self.data_processor.validate_columns_by_list(mapped_columns, logic_type)
            
            columns_info = {
                'original_columns': file_columns,
                'mapped_columns': mapped_columns,
                'column_mapping': col_map,
                'total_columns': len(file_columns),
                'preview_rows': len(preview_df)
            }
            
            if success:
                return True, f"Columns validation passed for {logic_type}", columns_info
            else:
                return False, validation_message, columns_info
                
        except Exception as e:
            return False, f"Error previewing file: {str(e)}", None

    def read_excel_file(self, file_path, logic_type):
        """
        ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel ‡∏´‡∏£‡∏∑‡∏≠ CSV ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        
        Args:
            file_path: ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå
            logic_type: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
            ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        """
        try:
            # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï log flags ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà
            self.data_processor._reset_log_flags()
            
            # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ Performance Optimizer
            if file_path.lower().endswith('.csv'):
                file_type = 'csv'
            elif file_path.lower().endswith('.xls'):
                file_type = 'excel_xls'
            else:
                file_type = 'excel'
            
            success, df = self.performance_optimizer.read_large_file_chunked(file_path, file_type)
            if not success:
                return False, "Unable to read file"
            
            # Apply column mapping (‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á mapping ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö header ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå)
            col_map = self.file_reader.build_rename_mapping_for_dataframe(df.columns, logic_type)
            if col_map:
                self.log_callback(f"üîÑ Renamed columns by mapping ({len(col_map)} columns)")
                df.rename(columns=col_map, inplace=True)
            
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á memory usage
            df = self.performance_optimizer.optimize_memory_usage(df)
            
            # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏ô staging table ‡∏î‡πâ‡∏ß‡∏¢ SQL ‡πÅ‡∏ó‡∏ô pandas
            self.log_callback(f"üîÑ Ingest as NVARCHAR(MAX) first, then validate/convert using SQL")
            
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î memory
            self.performance_optimizer.cleanup_memory()
            
            self.log_callback(f"üéâ File processing completed")
            return True, df
            
        except Exception as e:
            error_msg = f"‚ùå Error while reading file: {e}"
            self.log_callback(error_msg)
            return False, error_msg
    
    # ========================
    # Delegation Methods
    # ========================
    
    def set_search_path(self, path):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå Excel"""
        self.search_path = path
        self.file_reader.set_search_path(path)

    def find_data_files(self):
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå Excel ‡πÅ‡∏•‡∏∞ CSV ‡πÉ‡∏ô path ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î"""
        return self.file_reader.find_data_files()

    def detect_file_type(self, file_path):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå"""
        return self.file_reader.detect_file_type(file_path)

    def get_column_name_mapping(self, file_type):
        """‡∏£‡∏±‡∏ö mapping ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå"""
        return self.file_reader.get_column_name_mapping(file_type)

    def get_required_dtypes(self, file_type):
        """‡∏£‡∏±‡∏ö dtype ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå"""
        return self.data_processor.get_required_dtypes(file_type)

    def validate_columns(self, df, logic_type):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô"""
        return self.data_processor.validate_columns(df, logic_type)

    def comprehensive_data_validation(self, df, logic_type):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•"""
        return self.data_processor.comprehensive_data_validation(df, logic_type)

    def check_invalid_numeric(self, df, logic_type):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç"""
        return self.data_processor.check_invalid_numeric(df, logic_type)

    def generate_pre_processing_report(self, df, logic_type):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡πà‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (deprecated - ‡πÉ‡∏ä‡πâ SQL validation ‡πÅ‡∏ó‡∏ô)"""
        return self.data_processor.generate_pre_processing_report(df, logic_type)

    def apply_dtypes(self, df, file_type):
        """‡πÅ‡∏õ‡∏•‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤"""
        return self.data_processor.apply_dtypes(df, file_type)

    def move_uploaded_files(self, file_paths, logic_types=None):
        """‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå Uploaded_Files"""
        return self.file_manager.move_uploaded_files(file_paths, logic_types, self.search_path)

    # ========================
    # Legacy Methods (‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠ backward compatibility)
    # ========================
    
    def get_required_columns(self, file_type):
        """(Deprecated) ‡πÉ‡∏ä‡πâ get_required_dtypes ‡πÅ‡∏ó‡∏ô"""
        return self.data_processor.get_required_dtypes(file_type)

    def standardize_column_name(self, col_name):
        """‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô"""
        return self.file_reader.standardize_column_name(col_name)

    def normalize_col(self, col):
        """‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£ normalize column"""
        return self.file_reader.normalize_col(col)

    def load_settings(self):
        """‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å SettingsManager"""
        # ‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å SettingsManager
        settings_manager.load_all_settings()
        
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô file_reader ‡πÅ‡∏•‡∏∞ data_processor
        self.file_reader.column_settings = settings_manager.column_settings
        self.file_reader.dtype_settings = settings_manager.dtype_settings
        self.file_reader._settings_loaded = True
        
        self.data_processor.column_settings = settings_manager.column_settings  
        self.data_processor.dtype_settings = settings_manager.dtype_settings
        self.data_processor._settings_loaded = True
        
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï reference ‡πÉ‡∏ô FileService
        self.column_settings = settings_manager.column_settings

    def _process_dataframe_in_chunks(self, df, process_func, logic_type, chunk_size=5000):
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• DataFrame ‡πÅ‡∏ö‡∏ö chunk (legacy wrapper)"""
        return self.data_processor.process_dataframe_in_chunks(df, process_func, logic_type, chunk_size)

    def _reset_log_flags(self):
        """‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï log flags (legacy wrapper)"""
        self.data_processor._reset_log_flags()

    def clean_numeric_columns(self, df, file_type):
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç"""
        return self.data_processor.clean_numeric_columns(df, file_type)

    def truncate_long_strings(self, df, logic_type):
        """‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• string ‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏≥‡∏´‡∏ô‡∏î"""
        return self.data_processor.truncate_long_strings(df, logic_type)


    def upload_data_with_auto_schema_update(self, df, logic_type, processing_report=None, schema_name='bronze'):
        """
        ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó schema ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        
        Args:
            df: DataFrame ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î
            logic_type: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
            processing_report: (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)
            schema_name: ‡∏ä‡∏∑‡πà‡∏≠ schema ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            
        Returns:
            Tuple[bool, str]: (‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà, ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå)
        """
        try:
            from services.database_service import DatabaseService
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á database service
            db_service = DatabaseService()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
            connection_ok, conn_msg = db_service.check_connection()
            if not connection_ok:
                return False, f"Unable to connect to database: {conn_msg}"
            
            # ‡πÑ‡∏î‡πâ required columns ‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
            required_cols = self.get_required_dtypes(logic_type)
            if not required_cols:
                return False, "Data type settings not found"
            
            # ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö auto-fix ‡∏≠‡∏µ‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ ‡∏à‡∏∂‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ force_recreate ‡∏à‡∏≤‡∏Å processing_report
            force_recreate = False
            
            # ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            success, upload_msg = db_service.upload_data(
                df=df,
                logic_type=logic_type,
                required_cols=required_cols,
                schema_name=schema_name,
                log_func=self.log_callback,
                force_recreate=force_recreate
            )
            
            return success, upload_msg
            
        except Exception as e:
            error_msg = f"‚ùå Error during upload: {e}"
            self.log_callback(error_msg)
            return False, error_msg


    def print_detailed_validation_report(self, df, logic_type):
        """‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î (legacy method)"""
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
        self.log_callback("\n" + "="*80)
        self.log_callback("üîç Detailed data validation report")
        self.log_callback("="*80)
        
        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        self.log_callback(f"üìä Basic info:")
        self.log_callback(f"   ‚Ä¢ Total rows: {len(df):,}")
        self.log_callback(f"   ‚Ä¢ Total columns: {len(df.columns)}")
        self.log_callback(f"   ‚Ä¢ File type: {logic_type}")
        
        # ‡πÉ‡∏ä‡πâ comprehensive validation
        validation_result = self.comprehensive_data_validation(df, logic_type)
        
        if validation_result['summary']:
            self.log_callback("\nüìã Validation summary:")
            for msg in validation_result['summary']:
                self.log_callback(f"   ‚Ä¢ {msg}")
        
        self.log_callback("="*80)
